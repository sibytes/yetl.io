{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"YETL Yet Another ETL Framework"},{"location":"#yet-another-etl-framework","title":"Yet (another) ETL Framework","text":"<p>Expressive, agile, fun for Data Engineers using Python!</p> <p>Yetl is a configuration API for Databricks datalake house pipelines. It allows you to easily define the configuration and metadata of pipelines that can be accessed using a python to build modular pyspark pipelines with good software engineering practices.</p> <pre><code>pip install yetl-framework\n</code></pre>"},{"location":"#what-does-it-do","title":"What does it do?","text":"<p>Using yetl a data engineer can define configuration to take data from a source dataset to a destination dataset and just code the transform in between. It takes care of the mundane allowing the engineer to focus only on the value end of data flow in a fun and expressive way.</p> <p>Feaures:</p> <ul> <li>Define table metadata, properties and dependencies easily that can be used programmaticaly with pipelines</li> <li>Define table metadata in Excel and covert to a configuration yaml file using the CLI</li> <li>Creates Delta Lake databases and tables during the pipeline:<ul> <li>Delclare SQL files with create table statements in the SQL dir</li> <li>Dynamically infer the table SQL from the config and the schema of a data frame</li> <li>Initialise a Delta Table with no schema and the load it with merge schema true.</li> </ul> </li> <li>Create spark schema for schema of read</li> <li>Once you have a schema or data frame sometime it's handy to have the DDL e.g. for schema hints, the API will work this out for you</li> <li>API provides a table collection index and table mappings API with property acceessors for each table to use as you wish when building the pipeline</li> <li>Supports jinja variables for expressive configuration idioms</li> <li>Provides a timeslice object for parameterising pipelines with wildcard paths</li> <li>Provides timeslice transform to parse the datetime from a path or filename into the dataset</li> <li>Can be used to create checkpoints in a consistent ways for your project for complex streaming patterns</li> </ul> <p>Once you have re-usable modular pipeline code and configuration... you can get really creative:</p> <ul> <li>Parameterise whether you want to run your pipelines as batch or streaming</li> <li>Bulk load a migration and then switch to batch streaming autoloader for incrementals</li> <li>Generate databricks worflows</li> <li>Generate databricks DLT pipelines</li> <li>Parameterise bulk reload or incremental pipelines</li> <li>Test driven development</li> <li>Integrate with data expectations framework</li> </ul>"},{"location":"#what-is-it-really","title":"What is it really?","text":"<p>The best way to see what it is, is to look at a simple example.</p> <p>Define your tables:</p> <pre><code>version: 3.0.0\n\naudit_control:\n  delta_lake:\n    raw_dbx_patterns_control:\n      catalog: hub\n      header_footer:\n        sql: ../sql/{{database}}/{{table}}.sql\n        depends_on:\n          - raw.raw_dbx_patterns.*\n      raw_audit:\n        # if declaring your tables using SQL you can declare that SQL in a linked file. \n        # Jinja templating is supported for catalog, database and table\n        sql: ../sql/{{database}}/{{table}}.sql\n        depends_on:\n          # when depending on all the tables in a db you can use the wild card\n          # note these layers supported audit_control, landing, raw, base, curated\n          # layer.db.*\n          - raw.raw_dbx_patterns.*\n          # or to declare specifically \n          # layer.db.*\n          - audit_control.raw_dbx_patterns_control.header_footer\n\n# landing is just a reference to source files\n# there are no properties of interest but the database\n# name and tables themselves\nlanding:\n  # read declares these as tables that read\n  # using spark.read api and are not deltalake table\n  # these are typically source files of a specific format\n  # landed in blob storage.\n  read:\n    landing_dbx_patterns:\n      catalog: hub\n      customer_details_1: null\n      customer_details_2: null\n\n# raw is the 1st ingest layer of delta lake tables\n# raw is optional you may not need it if you data feed provided is very clean.\nraw:\n  delta_lake:\n    raw_dbx_patterns:\n      catalog: hub\n      customers:\n        id: id\n        depends_on:\n          - landing.landing_dbx_patterns.customer_details_1\n          - landing.landing_dbx_patterns.customer_details_2\n        # we can define tables level expected thresholds for\n        # schema on read errors from landing.\n        # There are warning and exception thresholds\n        # how what behaviour they drive in the pipeline \n        # is left to data engineer.\n        warning_thresholds:\n          invalid_ratio: 0.1\n          invalid_rows: 0\n          max_rows: 100\n          min_rows: 5\n        exception_thresholds:\n          invalid_ratio: 0.2\n          invalid_rows: 2\n          max_rows: 1000\n          min_rows: 0\n        # you can define any custome properties that you want\n        # properties provided can be used a filter when looking up\n        # tables in the API collection\n        custom_properties:\n          process_group: 1\n        # there are may other properties supported for delta lake tables\n        # e.g. zorder, partition by, cluster by, \n        # delta table properties, vacuum, auto increment id column\n        # see reference docs\u00df for this yaml spec\n        z_order_by:\n          - _load_date_1\n          - _load_date_2\n        vacuum: 30\n\nbase:\n  delta_lake:\n    # delta table properties can be set at stage level or table level\n    delta_properties:\n      delta.appendOnly: true\n      delta.autoOptimize.autoCompact: true    \n      delta.autoOptimize.optimizeWrite: true  \n      delta.enableChangeDataFeed: false\n    base_dbx_patterns:\n      catalog: hub\n      customer_details_1:\n        id: id\n        depends_on:\n          - raw.raw_dbx_patterns.customers\n        # delta table properties can be set at stage level or table level\n        # table level properties will overwride stage level properties\n        delta_properties:\n            delta.enableChangeDataFeed: true\n      customer_details_2:\n        id: id\n        depends_on:\n          - raw.raw_dbx_patterns.customers\n</code></pre> <p>Define you load configuration:</p> <pre><code>version: 3.0.0\ntables: ./tables.yaml\n\naudit_control:\n  delta_lake:\n    # delta table properties can be set at stage level or table level\n    delta_properties:\n        delta.appendOnly: true\n        delta.autoOptimize.autoCompact: true\n        delta.autoOptimize.optimizeWrite: true\n    managed: false\n    container: datalake\n    location: /mnt/{{container}}/data/raw\n    path: \"{{database}}/{{table}}\"\n    options:\n      checkpointLocation: \"/mnt/{{container}}/checkpoint/{{project}}/{{checkpoint}}\"\n\nlanding:\n  read:\n    trigger: customerdetailscomplete-{{filename_date_format}}*.flg\n    trigger_type: file\n    container: datalake\n    location: \"/mnt/{{container}}/data/landing/dbx_patterns/{{table}}/{{path_date_format}}\"\n    filename: \"{{table}}-{{filename_date_format}}*.csv\"\n    filename_date_format: \"%Y%m%d\"\n    path_date_format: \"%Y%m%d\"\n    # injects the time period column into the dataset\n    # using either the path_date_format or the filename_date_format\n    # as you specify\n    slice_date: filename_date_format\n    slice_date_column_name: _slice_date\n    format: cloudFiles\n    spark_schema: ../schema/{{table.lower()}}.yaml\n    options:\n      # autoloader\n      cloudFiles.format: csv\n      cloudFiles.schemaLocation:  /mnt/{{container}}/checkpoint/{{project}}/{{checkpoint}}\n      cloudFiles.useIncrementalListing: auto\n      # schema\n      inferSchema: false\n      enforceSchema: true\n      columnNameOfCorruptRecord: _corrupt_record\n      # csv\n      header: false\n      mode: PERMISSIVE\n      encoding: windows-1252\n      delimiter: \",\"\n      escape: '\"'\n      nullValue: \"\"\n      quote: '\"'\n      emptyValue: \"\"\n\nraw:\n  delta_lake:\n    # delta table properties can be set at stage level or table level\n    delta_properties:\n      delta.appendOnly: true\n      delta.autoOptimize.autoCompact: true    \n      delta.autoOptimize.optimizeWrite: true  \n      delta.enableChangeDataFeed: false\n    managed: false\n    container: datalake\n    location: /mnt/{{container}}/data/raw\n    path: \"{{database}}/{{table}}\"\n    options:\n      mergeSchema: true\n      checkpointLocation: \"/mnt/{{container}}/checkpoint/{{project}}/{{checkpoint}}\"\n\nbase:\n  delta_lake:\n    container: datalake\n    location: /mnt/{{container}}/data/base\n    path: \"{{database}}/{{table}}\"\n    options: null\n</code></pre> <p>Import the config objects into you pipeline:</p> <pre><code>from yetl import Config, Timeslice, StageType, Read, DeltaLake\n\npipeline = \"autoloader\"\nconfig_path = \"./test/config\"\nproject = \"test_project\"\ntimeslice = Timeslice(day=\"*\", month=\"*\", year=\"*\")\nconfig = Config(\n    project=project, pipeline=pipeline, config_path=config_path, timeslice=timeslice\n)\ntable_mapping = config.get_table_mapping(\n    stage=StageType.raw, table=\"customers\"\n)\n\nsource: Read = table_mapping.source[\"customer_details_1\"]\ndestination: DeltaLake = table_mapping.destination\nconfig.set_checkpoint(source=source, destination=destination)\n\nprint(table_mapping)\n</code></pre> <p>Use even less code and use the decorator pattern:</p> <pre><code>@yetl_flow(\n        project=\"test_project\", \n        stage=StageType.raw, \n        config_path=\"./test/config\"\n)\ndef autoloader(table_mapping:TableMapping):\n    # &lt;&lt; ADD YOUR PIPELINE LOGIC HERE - USING TABLE MAPPING CONFIG &gt;&gt;\n    return table_mapping # return whatever you want here.\n\nresult = autoloader(table=\"customers\")\n</code></pre>"},{"location":"#example-project","title":"Example project","text":"<p>databricks-patterns</p> <p>This example projects has 4 projects loading data landed from:</p> <ul> <li>adventure works</li> <li>adventure works lt</li> <li>adventure works dw</li> <li>header_footer - a small demo of files with semic structured headers and footers that stripped into an audit table on the when loaded.</li> <li>header_footer_uc - same as header footer but using databricks Unity Catalog.</li> </ul> Yet Another ETL Framework"},{"location":"about/","title":"About","text":"YETL Yet Another ETL Framework <p>Made by Shaun Ryan Yet Another ETL Framework"},{"location":"on-board-datafeed/","title":"On-Board Datafeed","text":"YETL Yet Another ETL Framework"},{"location":"on-board-datafeed/#on-board-datafeed","title":"On-Board Datafeed","text":"<p>These are the outline steps to on-board a new datafeed:</p> <ol> <li>Load Landing Data</li> <li>Create Yetl Project</li> <li>Create Table Metadata</li> <li>Create Pipeline Metadata</li> <li>Create Spark Schema</li> <li>Develop &amp; Test Pipeline</li> <li>Deploy Product</li> </ol>"},{"location":"on-board-datafeed/#load-landing-data","title":"Load Landing Data","text":"<p>This project is about loading data from cloud blob into databricks deltalake tables. Data would normally be orchestrated into landing using some other tool for example Azure Data Factory or AWS Data Pipeline or some other tool that specialises in securely wholesale copying datasets into the cloud or between cloud services.</p> <p>You may already have this orchestration in place in which case the data will in your landing blob location already or you mock it by getting a sample of data and manually copying it into the location it will be landed to.</p> <p>For test driven development you can include a small vanilla hand crafted data set into the project itself that can automatically be copied into place from the workspace files as way of creating repeatable end to end integration tests that can staged, executed and torn down with simple commands.</p>"},{"location":"on-board-datafeed/#create-yetl-project","title":"Create Yetl Project","text":"<p>Create a directory, setup virtual python environment and install yetl.</p> <pre><code>mkdir my_project\ncd my_project\npython -m venv venv\nsource venv/bin/activate\npip install yetl-framework\n</code></pre> <p>Create yetl project scaffolding.</p> <pre><code>python -m yetl init my_project \n</code></pre>"},{"location":"on-board-datafeed/#create-table-metadata","title":"Create Table Metadata","text":"<p>Fill out the spreadsheet template with landing and deltalake architecture that you want to load. The excel file has to be in a specific format other the import will not work. Use this example as a template</p> <p>NOTE:</p> <ul> <li>Lists are entered using character return between items in an Excel cell.</li> <li>Dicts are entered using a : between key value pairs and character returns betweenitmes in an Excel cell.</li> </ul> merge_column.column required type description stage y [audit_control, landing, raw, base, curated] The architecural layer of the data lake house you want the DB in table_type y [read, delta_lake] What type of table to create, read is a spark.read, delta_lake is a delta table. catalog n str name of the catalog. Although you can set it here in the condif the api allows passing it as a parameter also. database y str name of the database table y str name of the table sql n [y, n] whether or not to include a default link to a SQL ddl file for creating the table. id n str, List[str] a column name or list of column names that is the primary key of the table. depends_on n List[str] list of other tables that the table is loaded from thus creating a mapping. It required the yetl index which is <code>stage.database.table</code> you can also use  <code>stage.database.*</code> for exmaple if you want to reference all the tables in a database. deltalake.delta_properties n Dict[str,str] key value pairs of databricks delta properties deltalake.identity n [y, n] whether or not to include an indentity on the table when a delta table is created implicitly deltalake.partition_by n str, List[str] column or list of columns to partition the table by deltalake.delta_constraints n Dict[str,str] key value pairs of delta table constraints deltalake.z_order_by n str, List[str] column or list of columns to z-order the table by deltalake.vacuum n int vaccum threshold in days for a delta table warning_thresholds.invalid_ratio n float ratio of invalid to valid rows threshold that can be used to raise a warning warning_thresholds.invalid_rows n int number of invalid rows threshold that can be used to raise a warning warning_thresholds.max_rows n int max number of rows thresholds that can be used to raise a warning warning_thresholds.mins_rows n int min number of rows thresholds that can be used to raise a warning error_thresholds.invalid_ratio n float ratio of invalid to valid rows threshold that can be used to raise an exception error_thresholds.invalid_rows n int number of invalid rows threshold that can be used to raise an exception error_thresholds.max_rows n int max number of rows thresholds that can be used to raise an exception error_thresholds.mins_rows n int min number of rows thresholds that can be used to raise an exception custom_properties.process_group n any customer properties can be what ever you want. Yetl is smart enough to build them into the API custom_properties.rentention_days n any custom_properties.anything_you_want n any <p>Create the tables.yaml file by executing:</p> <pre><code>python -m yetl import-tables ./my_project/pipelines/tables.xlsx ./my_project/pipelines/tables.yaml\n</code></pre>"},{"location":"on-board-datafeed/#create-pipeline-metadata","title":"Create Pipeline Metadata","text":"<p>In the <code>./my_project/pipelines</code> folder create a yaml file that contains the metadata specifying how to load the tables defined in <code>./my_project/pipelines/tables.yaml</code>. You can call them whatever you want and you can create more than one. Perhaps one that batch loads and another that event stream loads. The yetl api will allow you to parameterise which pipeline metadata you want to use. For the purpose of these docs we will refere to this pipeline as <code>my_pipeline.yaml</code>.</p> <p>The pipeline file <code>my_pipeline.yaml</code> has a relative file reference to <code>tables.yaml</code> and the therefore yetl knows what files to use to stitch the table metadata together.</p> <p>Please see the pipeline reference documentation for details. Here is an example.</p>"},{"location":"on-board-datafeed/#create-spark-schema","title":"Create Spark Schema","text":"<p>Once the yetl metadata is in place we can start using the API. The 1st task is to create the landing schema that need to load the data. This can be done using a simple notebook on databricks.</p> <p>Using databricks repo's you can clone your project into databricks.</p> <p>This must be in it's own cell: <pre><code>%pip install yetl-framework==3.0.0\n</code></pre></p> <p>Executing the following code will load the files and save the spark schema into the `./my_project/schema' directory in yaml format making it easy to review and adjust if you wish. There's no reason to move the files anywhere else once created, yetl uses this location as a schema repo. The files will named after the tables making it intuitive to understand what the schema's are and how the map.</p> <p>The ad works example project shows this notebook approach working very well creating the schema over a relatively large number of tables.</p> <pre><code>from yetl import (\n  Config, Read, DeltaLake, Timeslice\n)\nimport yaml, os\n\ndef create_schema(\n  source:Read, \n  destination:DeltaLake\n):\n\n  options = source.options\n  options[\"inferSchema\"] = True\n  options[\"enforceSchema\"] = False\n\n  df = (\n    spark.read\n    .format(source.format)\n    .options(**options)\n    .load(source.path)\n  )\n\n  schema = yaml.safe_load(df.schema.json())\n  schema = yaml.safe_dump(schema, indent=4)\n\n  with open(source.spark_schema, \"w\", encoding=\"utf-8\") as f:\n    f.write(schema)\n\nproject = \"my_project\"\npipeline = \"my_pipeline\"\n\n# Timeslice may be required depending how you've configured you landing area.\n# here we just using a single period to define the schema \n# Timeslice(year=\"*\", month=\"*\", day=\"*\") would use all the data \n# you have which could be very inefficient.\n\n# This exmaple uses the data in the landing partition of 2023-01-01\n# how that is mapped to file and directories the my_pipeline definition\nconfig = Config(\n  project=project, \n  pipeline=pipeline,\n  timeslice=Timeslice(year=2023, month=1, day=1)\n)\n\ntables = config.tables.lookup_table(\n  stage=StageType.raw, \n  first_match=False\n)\n\nfor t in tables:\n  table_mapping = config.get_table_mapping(\n    t.stage, t.table, t.database, create_table=False\n  )\n  create_schema(table_mapping.source, table_mapping.destination)\n</code></pre> <p>As you can see using this approach can also be used for creating tables in a pipeline step prior to any load pipeline using the <code>create_table</code> parameter. It will either create explicity defined tables using SQL DML if you've configured any or just create register empty delta tables with no schema. This may be required if you have multiple sources flowing into a single table (fan-in) to avoid transaction isolation errors creating the tables the 1st time that the pipeline runs.</p>"},{"location":"on-board-datafeed/#develop-test-pipeline","title":"Develop &amp; Test Pipeline","text":"<p>TODO</p>"},{"location":"on-board-datafeed/#deploy-product","title":"Deploy Product","text":"<p>TODO</p> Yet Another ETL Framework"},{"location":"project/","title":"Project","text":"YETL Yet Another ETL Framework"},{"location":"project/#project","title":"Project","text":"<p>Yetl has the concept of project. A project houses all the assets to configure your pipelines, the pipelines themselves and deploymemnt assets. They are deployable products that can be a data feed or an individual component data feed product of something larger.</p> <p>Although it's entirely possible to reconfigure the project structure a canonical structure is recommended since the API just works without any additional consideration for all of these environments:</p> <ul> <li>Local</li> <li>Databricks Workspace</li> <li>Databricks Repo</li> </ul>"},{"location":"project/#create-a-new-project","title":"Create a new project","text":"<p>Yetl has a built-in cli for common tasks. One of those common tasks is creating a new project.</p> <p>Create a python virtual environment and install yetl:</p> <pre><code>mkdir yetl_test\ncd yetl_test\npython -m venv venv\nsource venv/bin/activate\npip install --upgrade pip\npip install yetl-framework\n</code></pre> <p>Create a new yetl project:</p> <pre><code>python -m yetl init test_yetl\n</code></pre> <p>This will create:</p> <ul> <li>logging configuration file <code>logging.yaml</code></li> <li>yetl project configuration file <code>yetl_test.yaml</code> project configuration file.</li> <li>Excel template <code>tables.xlsx</code> to curate tables names and properties that you want to load, this can be built into a configuration <code>tables.yaml</code> with <code>python -m yetl import-tables</code></li> <li><code>json-schema</code> contains json schema that you can reference and use redhat's yaml plugin when crafting configuration to get intellisense and validation. There are schemas for:<ul> <li><code>./pipelines/project.yaml</code></li> <li><code>./pipelines/tables.yaml</code></li> <li><code>./pipelines/&lt;my_pipelines&gt;.yaml</code></li> </ul> </li> <li>directory structure to build pipelines</li> </ul> <p></p> <ul> <li>databricks - will store databricks workflows, notebooks, etc that will be deployed to databricks.</li> <li>pipelines - will store yetl configuration for the data pipelines.</li> <li>sql - if you choose to define some tables using SQL explicitly and have yetl create those tables then the SQL files will go in here.</li> <li>schema - will hold the spark schema's in yaml for the landing data files when we autogenerate the schema's</li> </ul>"},{"location":"project/#project-config","title":"Project config","text":"<p>The <code>yetl_test.yaml</code> project file tells yetl where to find these folders. It's best to leave it on the defaults:</p> <pre><code>databricks_notebooks: ./databricks/notebooks\ndatabricks_queries: ./databricks/queries\ndatabricks_workflows: ./databricks/workflows\nname: test_yetl\npipeline: ./pipelines\nspark:\n    config:\n        spark.databricks.delta.allowArbitraryProperties.enabled: true\n        spark.master: local\n        spark.sql.catalog.spark_catalog: org.apache.spark.sql.delta.catalog.DeltaCatalog\n        spark.sql.extensions: io.delta.sql.DeltaSparkSessionExtension\n    logging_level: ERROR\nspark_schema: ./schema\nsql: ./sql\nversion: 3.0.0\n</code></pre> <p>It also holds a spark configuration, this is what yetl uses to create the spark session if you choose to run yetl code locally. This can be usefull for local development and testing in deveops pipelines. Since we are just leveraging pyspark and delta lake it's very easy to install a local spark environment however you will need java installed if you want to work this way.</p> <p>The version attribute is also important this makes sure that you're using the right configuration schema for the specific version of yetl you're using. If these do not agree to the minor version then yetl will raise version incompatability errors. This ensures it's clear the metadata clearly denotes the version of yetl the it requires. Under the covers yetl uses pydantic to ensure that metadata is correct when deserialized.</p>"},{"location":"project/#logging-config","title":"Logging config","text":"<p>The <code>logging.yaml</code> is the python logging configuration. Configure yetl logging as you please by altering this file.</p>"},{"location":"project/#sql","title":"SQL","text":"<p>Sometimes there are requirements to explicitly create tables using SQL DDL statements. </p> <p>For example if you're loading a fan in pattern where multiple source tables are loading into the same destination table in parallel you cannot create the schema from the datafeed on the fly, either with or without the merge schema option. This is because delta tables are optimistically isolated and attempting the change the schema in multiple process will mostly likely cause a conflict. This is common pattern for audit tracking tables when loading bronze and silver tables (raw and base stages respectively)</p> <p>You may also just find it more practical to manage table creation by explicitly declaring create table SQL statements. You can also use this feature to create views.</p> <p>In order to explicitly define tables or views using SQL:</p> <ol> <li>Create a folder in the <code>sql</code> directory that is the database name e.g. <code>my_database</code></li> <li>Create a .sql file in the folder called the name of the table e.g. <code>my_table.sql</code></li> <li>Put a deltalake compatible create SQL statement in the sql file</li> </ol> <p>e.g.</p> <pre><code>./sql/my_database/my_table.sql\n</code></pre> <p>For example, in this project we use this feature to create an audit tracking table for loading a ronze (raw) tables called <code>yetl_control_header_footer.raw_audit.sql</code>. </p> <p>Note that if we want to declare the table unmanaged and provide an explicit location we can do using the jinja variable <code>{{location}}</code> which is defined in the pipeline configuration. See the SQL  documentation for full details and complete list of jinja variables supported.</p>"},{"location":"project/#pipeline","title":"Pipeline","text":"<p>The pipline folder holds yaml files that describe the databases, tables, files and pipeline metadata that the yetl API will stitch, render and deserialize into an api that can be used to construct data pipelines using pyspark.</p> <p>A lot of thought has gone into this design to make configuration creation and management as developer friendly and minimal as possible. There are 2 core types of files:</p> <ol> <li>Tables &amp; files - tables.yaml contains WHAT we are loading</li> <li>Pipeline - {pipeline_pattern_name}.yaml contains HOW we are loading it</li> </ol>"},{"location":"project/#tables-files-the-what","title":"Tables &amp; Files - The What!","text":"<p>This configuration file contains the information of WHAT the database tables and files are that we want to load. What we mean by that, is that it's at the grain of the table and therefore each table can be configured differently if required. For example each table has it's own name. It doesn't contain any configuration of how the table is loaded.</p> <p>One table definition file is allowed per project. Each file can hold definitions for any number of tables for databases at the following data stages. Database table type can be <code>read</code> (spark read api for various formats) or <code>delta_lake</code>:</p> <ul> <li><code>audit_control</code> - <code>delta_lake</code></li> <li><code>source</code> - <code>read</code> or <code>delta-lake</code></li> <li><code>landing</code> - <code>read</code></li> <li><code>raw</code> (bronze) - <code>delta-lake</code></li> <li><code>base</code> (silver) - <code>delta-lake</code></li> <li><code>curated</code> (gold) - <code>delta-lake</code></li> </ul> <p>Typically <code>read</code> table types would be used in <code>landing</code> and <code>delta-lake</code> tables for everything else. <code>source</code> may not normally be required for Databricks pipelines since data will be landed using some other cross domain data orchestration tool e.g. Azure Data Factory. However yetl does support <code>source</code> for <code>read</code> and <code>delta-lake</code> tables types it can be usefull when:</p> <ul> <li>data doesn't need to be landed and is available from some via a spark source</li> <li>migrating tables from an older deltalake into a new architecute e.g. Unity Catalog migration</li> <li>landing data directly from api's</li> </ul> <p>It is therefore opinionated about the basic levels that should be implemented in terms of stages and follows the medallion lakehouse arhitecture. It's rigidity starts and ends there however; you can put what ever tables you want in any layer, in any datbase with dependencies entirely of your own choosing.</p> <p>tables.yaml example</p> <p>By far the biggest hurdle is defining the table configuration. To rememedy this the yetl CLI tool has command to convert an Excel document into the required yaml format. Curating this information in Excel is far more user friendly than a yaml document for large numbers of tables. See the pipeline documentation for full details.</p> <pre><code>python -m yetl import-tables ./pipelines/tables.xlsx ./pipelines/tables.yaml\n</code></pre> <p>tables.xlsx -&gt; tables.yaml</p>"},{"location":"project/#pipeline-the-how","title":"Pipeline - The How!","text":"<p>This configuration file contains pipeline configuration properties that describes HOW the tables are loaded. For a given feed that is landed and loaded to silver (base) tables a lot of these are the same for every table in the feed. There can be more than one of these files for a given project. In other words you can define more than one way to load your tables and parameterise the pattern in the pipeline.</p> <p>An example use case of this might be to batch load a large migration dataset; then turn on autoloader and event load incrementals from that position onwards.</p> <p>batch.yaml pipeline example</p>"},{"location":"project/#summary","title":"Summary","text":"<p>So what does yetl do that provides value, since all we have is a bunch of configuration files:</p> <ol> <li>Configuration Stitching</li> <li>Jinja Rendering</li> <li>Validation</li> <li>API</li> </ol>"},{"location":"project/#configuration-stitching","title":"Configuration Stitching","text":"<p>It stitches the table and pipeline data together using a minimal python idioms and provides easy access to the table collections and pipeline properties. Some properties fall on a gray line in the sense that sometimes they are the same for all tables but on specific occaisions you might want them to be different. For example deltalake properties. You can define them in the pipeline doc but override them if you want on specific tables.</p>"},{"location":"project/#jinja-rendering","title":"Jinja Rendering","text":"<p>There are some jinja templating and features built into the configuration deserialization. This can avoid repetition allowing for easier maintenance; or provide enhanced functions around injecting time slice parameters of a specific format into file paths. See the pipeline documentation for specific details.</p>"},{"location":"project/#validation","title":"Validation","text":"<p>When yetl uses pydantic to validate and desrialize the configuration data making it robust, easy to use and easy to extend and support.</p>"},{"location":"project/#api","title":"API","text":"<p>The yetl API exposes the validated, stitched and rendered configuration into an easy to use API that can used to implement modular pipelines exactly how you want to. Yetl does not take over specifically the way engineers want to build and test their pipelines.</p> <p>It does however provide an easy to use API to iterate and access the metadata you need. The tables are indexed and can easily be retrieved or iterated. So you can build your custom pyspark pipelines or even just ignore some of the managed table properties and iterate the tables to create DLT pipelines in databricks.</p> Yet Another ETL Framework"},{"location":"reference/pipeline-config/","title":"Pipeline config","text":"YETL Yet Another ETL Framework Yet Another ETL Framework"},{"location":"reference/project-config/","title":"Project config","text":"YETL Yet Another ETL Framework Yet Another ETL Framework"},{"location":"reference/python-api/","title":"Python API","text":"YETL Yet Another ETL Framework"},{"location":"reference/python-api/#python-api","title":"Python API","text":"<p>The goal of yetl is to easily declare data pipeline configuration. Once declared you'll want to access that data strcutures that you've declared using python. This reference shows the canonical patterns to load up the configuration and use it however you see fit.</p>"},{"location":"reference/python-api/#initilise","title":"Initilise","text":"<p>Each time you want to access the configuration using the API you will need to deserialize the configuration into python objects held in memory.</p>"},{"location":"reference/python-api/#load-configuration","title":"Load Configuration","text":"<p>The following example loads the configuration from the following configuration files into an instance of the Config object:</p> <ul> <li>./my_project/pipelines/tables.yaml</li> <li>./my_project/pipelines/batch.yaml</li> </ul> <pre><code>from yetl import (\n  Config, StageType\n)\n\nconfig = Config(\n  project=\"my_project\", \n  pipeline=\"batch\"\n)\n</code></pre> <p>The full Config constructor has the following arguments:</p> <ul> <li><code>project: str</code></li> </ul> <p>Name for the project.</p> <ul> <li><code>pipeline: str</code></li> </ul> <p>Name of the pipeline config file without the extension that has the config you want to use.</p> <ul> <li><code>timeslice: Timeslice = None</code></li> </ul> <p>The timeslice that you want to inject into the config and replace the date time mask jinja variables in the configuration (see the Timeslice section). The timeslice is optional since you may just want to pull back a collection of delta tables for operations other than loading data or load the current UTC datetime which is the default.</p> <ul> <li><code>config_path: str = None</code></li> </ul> <p>Yetl will do it's best to figure out where the configuration is located based on your project file confg, operating env and the project configuration file. If you're not using the standard settings for whatever reason you can provide an explicit path to where youtr configuration resides.</p>"},{"location":"reference/python-api/#inject-timeslice","title":"Inject Timeslice","text":"<p>Yetl provides a timeslice object for the convenience of injecting time periods into custom formats on data file paths and file names. For example it's typical to land data files partitioned as follows:</p> <pre><code>/Volumes/development/my_project/customer/2023/07/30/customer-20230730.json\n</code></pre> <p>In the pipeline configuration we generalise this by declaring the following expression definition. This tells yetl where to insert the timeslice using what datetime format. The datatime format is expressed as a python datetime format. There are 2 formats because the filename and path datetime format can be different:</p> <pre><code>    location: \"/Volumes/{{catalog}}/my_project/{{table}}/{{path_date_format}}\"\n    filename: \"{{table}}-{{filename_date_format}}*.json\"\n    filename_date_format: \"%Y%m%d\"\n    path_date_format: \"%Y/%m/%d\"\n</code></pre> <p>In some loading patterns we need to inject the period of data we want to load into the config. The <code>Timeslice</code> object is provided specifically to do this since it internally handles datetime formatting, validation and wildcard handling for bulk data loading.</p>"},{"location":"reference/python-api/#specific-day","title":"Specific Day","text":"<p>For example loading a specific day, injecting this <code>Timeslice</code>:</p> <pre><code>from yetl import (\n  Config, Timeslice\n)\n\nconfig = Config(\n  project = \"my_project\",\n  timeslice = Timeslice(year=2023, month=7, day=30)\n  pipeline = \"batch\"\n)\n</code></pre> <p>Will result in this path: <pre><code>/Volumes/development/my_project/customer/2023/07/30/customer-20230730*.json\n</code></pre></p>"},{"location":"reference/python-api/#partial-bulk","title":"Partial Bulk","text":"<p>For example bulk loading a year, injecting this <code>Timeslice</code>:</p> <pre><code>from yetl import (\n  Config, Timeslice\n)\n\nconfig = Config(\n  project = \"my_project\",\n  timeslice = Timeslice(year=2023, month=\"*\", day=\"*\")\n  pipeline = \"batch\"\n)\n</code></pre> <p>Will result in this wildcard path: <pre><code>/Volumes/development/my_project/customer/2023/*/*/customer-2023***.json\n</code></pre></p>"},{"location":"reference/python-api/#full-bulk","title":"Full Bulk","text":"<p>For example all time, injecting this <code>Timeslice</code>:</p> <pre><code>from yetl import (\n  Config, Timeslice\n)\n\nconfig = Config(\n  project = \"my_project\",\n  timeslice = Timeslice(year=\"*\", month=\"*\", day=\"*\")\n  pipeline = \"batch\"\n)\n</code></pre> <p>Will result in this wildcard path: <pre><code>/Volumes/development/my_project/customer/*/*/*/customer-****.json\n</code></pre></p> <p>Note: </p> <p>If you're stream loading using databricks cloud files and trigger now, you don't need to worry about timeslice loading your data since databricks will automatically track and checkpoint the files that you're loading. However batch stream loading also has some downsides, but not worry since yetl has you covered making it easy to inject timeslice loading.</p> Yet Another ETL Framework"},{"location":"reference/table-config/","title":"Table Config","text":"YETL Yet Another ETL Framework"},{"location":"reference/table-config/#table-configuration","title":"Table Configuration","text":"<p>The table configuration defines that is loaded in a data pipeline.</p> <p>The table metadata is the most difficult and time consuming metadata to curate. Therefore yetl provides a command line tool to convert an excel curated definition of metadata into the required yaml format. Curating this kind of detail for large numbers of tables is much easier to do in an Excel document due to it's excellent features.</p>"},{"location":"reference/table-config/#example","title":"Example","text":"<p>A solution lands 3 files:</p> <ul> <li><code>customer_details_1</code></li> <li><code>customer_details_2</code></li> <li><code>customer_preferences</code></li> </ul> <p>The files are loaded into raw from landing with a deltalake table for each file.</p> <p>Those tables are then loaded into base tables. <code>customer_details_1</code> and <code>customer_details_2</code> are unioned together and loaded into a customer table. So the base tables are:</p> <ul> <li><code>customers</code></li> <li><code>customer_perferences</code></li> </ul> <p>Each file has a header and footer with some audit data we load this with some other etl audit data into deltalake audit tables:</p> <ul> <li><code>header_footer</code></li> <li><code>raw_audit</code></li> <li><code>base_audit</code></li> </ul> <p>Here is the <code>tables.yaml</code> metadata that describes the stages, databases and tables:</p> <pre><code># yaml-language-server: $schema=./json_schema/sibytes_yetl_tables_schema.json\n\nversion: 3.0.0\n\naudit_control:\n  delta_lake:\n    yetl_control_header_footer_uc:\n      catalog: development\n      base_audit:\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.*\n        sql: ../sql/{{database}}/{{table}}.sql\n        vacuum: 168\n      header_footer:\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.*\n        sql: ../sql/{{database}}/{{table}}.sql\n        vacuum: 168\n      raw_audit:\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.*\n        - audit_control.yetl_control_header_footer_uc.header_footer\n        sql: ../sql/{{database}}/{{table}}.sql\n        vacuum: 168\n\nlanding:\n  read:\n    yetl_landing_header_footer_uc:\n      catalog: development\n      customer_details_1: null\n      customer_details_2: null\n      customer_preferences: null\n\nraw:\n  delta_lake:\n    yetl_raw_header_footer_uc:\n      catalog: development\n      customer_details_1:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - landing.yetl_landing_header_footer_uc.customer_details_1\n        exception_thresholds:\n          invalid_rows: 2\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n      customer_details_2:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - landing.yetl_landing_header_footer_uc.customer_details_2\n        exception_thresholds:\n          invalid_rows: 2\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n      customer_preferences:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - landing.yetl_landing_header_footer_uc.customer_preferences\n        exception_thresholds:\n          invalid_rows: 2\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n\nbase:\n  delta_lake:\n    yetl_base_header_footer_uc:\n      catalog: development\n      customer_details_1:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.customer_details_1\n        exception_thresholds:\n          invalid_rows: 0\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n      customer_details_2:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.customer_details_2\n        exception_thresholds:\n          invalid_rows: 0\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n      customer_preferences:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.customer_preferences\n        exception_thresholds:\n          invalid_rows: 0\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n      customers:\n        custom_properties:\n          process_group: 1\n          rentention_days: 365\n        depends_on:\n        - raw.yetl_raw_header_footer_uc.*\n        exception_thresholds:\n          invalid_rows: 0\n          min_rows: 1\n        id: id\n        vacuum: 168\n        z_order_by: _load_date\n</code></pre>"},{"location":"reference/table-config/#specification","title":"Specification","text":"<p>If you use the <code>yetl</code> cli to create a project using <code>python -m yetl init &lt;my_project&gt;</code> then the json validation schema for the config files including table the table config will be created at <code>./&lt;my_project&gt;/piplines/json-schemas/sibytes_yetl_tables_schema.json</code>. Using vscode and the RedHat yaml extension you can add the following json schema reference to <code>./&lt;my_project&gt;/piplines/tables.yml</code> to provide live validation and intellisense:</p> <pre><code># yaml-language-server: $schema=./json_schema/sibytes_yetl_tables_schema.json\n</code></pre> <p>This reference describes the required format of the <code>tables.yaml</code> configuration.</p> <pre><code>version: major.minor.patch\n\n&lt;stage:Stage&gt;:\n    &lt;table_type:TableType&gt;:\n        delta_properties:\n          &lt;property_name&gt;: str\n        &lt;database_name:string&gt;:\n            catalog: str|null\n            &lt;table_name:str&gt;:\n                id: str|list[str]|null\n                depends_on: index|list[index]|null\n                delta_properties:\n                  &lt;property_name&gt;: str\n                delta_constraints:\n                  &lt;constraint_name&gt;: str\n                custom_properties:\n                  &lt;property_names&gt;: str\n                z_order_by: str|list[str]|null\n                partition_by: str|list[str]|null\n                cluster_by: str|list[str]|null\n                vacuum: int|null\n                sql: path|null\n                warning_thresholds:\n                    invalid_ratio: float\n                    invalid_rows: int\n                    max_rows: int\n                    min_rows: int\n                exception_thresholds:\n                    invalid_ratio: float\n                    invalid_rows: int\n                    max_rows: int\n                    min_rows: int\n\n            &lt;table_name:str&gt;:\n              # table details\n              ...\n            &lt;table_name:str&gt;:\n              # table details\n              ...\n</code></pre>"},{"location":"reference/table-config/#version","title":"version","text":"<p>Version is the version number of yetl that the metadata is compatible with. If the major and minor version are not the same as the yetl python libary that you're using to load the metadata then an error will be raised. This is to ensure the metadata is compatible with the version of yetl that you're using.</p> <p>Example:</p> <pre><code>version: 3.0.0\n</code></pre>"},{"location":"reference/table-config/#stage","title":"Stage","text":"<p>The stage of the datalake house architecture. Yetl supports the following <code>stage</code>s:</p> <ul> <li><code>audit-control</code> - define tables for holding etl data and audit logs</li> <li><code>landing</code> - define landing object store where files are copied into you your cloud storage before they uploaded into the delta lakehouse</li> <li><code>raw</code> - define databases and tables for the bronze layer of the datalake. These will typically be deltalake tables loading with landing data with nothing more than schema validation applied</li> <li><code>base</code> - define databases and deltalake tables for the silver layer of the datalake. These tables will hold data loaded from raw with data quality and cleansing applied.</li> <li><code>curated</code> - define databases and deltalake tables for the gold layer of the datalake. These tables will hold the results of heavy transforms that integrate and aggregate data using complex business transformations specifically for business requirements.</li> </ul> <p>At least 2 stages must defined:</p> <ul> <li><code>landing</code></li> <li><code>raw</code></li> </ul> <p>These stages are optional:</p> <ul> <li><code>audit_control</code></li> <li><code>base</code></li> <li><code>curated</code></li> </ul> <p>Example:</p> <pre><code>audit_control:\n  delta_lake:\n...\nlanding:\n  read:\n...\nraw:\n  delta_lake:\n</code></pre>"},{"location":"reference/table-config/#delta_properties","title":"delta_properties","text":"<p>Deltalake properties is an object of key-value pairs that describes the deltalake properties. They can be defined at the table type level or the table level. The lowest level of granularity takes precedence over the higher levels. So you can define properties at a high level but override them at the table level if a table has specific properties that need to be defined. </p> <p>Example:</p> <pre><code>delta_properties:\n  delta.appendOnly: true\n  delta.autoOptimize.autoCompact: true    \n  delta.autoOptimize.optimizeWrite: true  \n  delta.enableChangeDataFeed: false\n</code></pre>"},{"location":"reference/table-config/#delta_constraints","title":"delta_constraints","text":"<p>Deltalake properties is an object of key-value pairs that describes the deltalake constraints. The key is the constraint name and the value is the sql constraint.</p> <p>The constraints are added when yetl creates the tables.</p> <pre><code>delta_properties:\n  dateWithinRange: \"(birthDate &gt; '1900-01-01')\"\n  validIds: \"(id &gt; 1 and id &lt; 99999999)\"\n</code></pre>"},{"location":"reference/table-config/#tabletype","title":"TableType","text":"<p>Table type is the type of table that is used. Yetl supports the following <code>table_type</code>s:</p> <ul> <li><code>read</code> - These are tables that are read using the spark read data api. Typically these are files with various formats. These types of tables are typically defined on the <code>landing</code> stage of the datalake.</li> <li><code>delta_lake</code> - These are deltalake tables that written to and read from during a pipeline load.</li> </ul> <p>Example:</p> <pre><code>audit_control:\n  delta_lake:\n...\nlanding:\n  read:\n...\nraw:\n  delta_lake:\n</code></pre>"},{"location":"reference/table-config/#index","title":"index","text":"<p>Index is a string formatted specifically to describe a table index. In the Yetl api the tables are index and the index can be used to quickly find and define dependencies.</p> <p>The index takes the following form:</p> <pre><code>stage.database.table\n</code></pre> <p>It supports a wild card form for defining or finding a collection of tables e.g.</p> <ul> <li><code>stage.*.*</code> - return/configure all the tables in a stage</li> <li><code>stage.database.*</code> - return/configure all the tables in a database</li> </ul> <p>Example:</p> <pre><code>audit_control:\n  delta_lake:\n    yetl_control_header_footer:\n      base_audit:\n        # this table depends on all the tables in the raw database called yetl_raw_header_footer\n        depends_on:\n        - raw.yetl_raw_header_footer.*\n</code></pre>"},{"location":"reference/table-config/#id","title":"id","text":"<p><code>id</code> is a string or list of strings that is the columns name or names of the table uniqie identifier.</p>"},{"location":"reference/table-config/#z_order_by","title":"z_order_by","text":"<p><code>z_order_by</code> is a string or list of strings that is the columns name or names to z_order the table by.</p>"},{"location":"reference/table-config/#partition_by","title":"partition_by","text":"<p><code>partition_by</code> is a string or list of strings that is the columns name or names to partition the table by.</p>"},{"location":"reference/table-config/#sql","title":"sql","text":"<p><code>sql</code> is relative path to the directory that holds a file container the explicit SQL to create the table. Note that jinja varaiable can be used for database and table thus defining that the sql directory is structured by database and table.</p> <p>Example:</p> <pre><code>sql: ../sql/{{database}}/{{table}}.sql\n</code></pre>"},{"location":"reference/table-config/#thresholds","title":"thresholds","text":"<p>Thresholds allow to define ETL audit metrics for each table. There are 2 properties for this:</p> <ul> <li>warning_thresholds -  used to define metrics that if exceeded raises a warning</li> <li>exception_thresholds - usde to define metrics that if exceeded raises an exception</li> </ul> <p>This is just metadata so how you use it and handle this metadata is entirely down to the developmer however. The pipeline code it self is used to calculate what these values are and compare them to the these thresholds and take appropriate action.</p> <p>Each threshold type supports the following metrics:</p> <ul> <li><code>invalid_ratio</code> - number of invalid records divided by the total number of records</li> <li><code>invalid_rows</code> - number of invalid records</li> <li><code>min_rows</code> - minimum number of rows</li> <li><code>max_rows</code> - maximum number of rows</li> </ul> <p>Example:</p> <pre><code>warning_thresholds:\n    # if more than 10% of the rows are invalid then raise a warning.\n    invalid_ratio: 0 \n    invalid_rows: 0\n    max_rows: null\n    # if there's less than 1000 records raise a warning\n    min_rows: 1000\nexception_thresholds:\n    # if more than 50% of the rows are invalid then raise an exception\n    invalid_ratio: 0.5\n    invalid_rows: null\n    max_rows: null\n    # if there's less than 1 record raise an exception\n    min_rows: 1\n</code></pre>"},{"location":"reference/table-config/#vacuum","title":"vacuum","text":"<p><code>vacuum</code> is the day threshold over which to apply the vacuum statement.</p>"},{"location":"reference/table-config/#custom_properties","title":"custom_properties","text":"<p><code>custom_properties</code> is object of key value pairs for anything that you want to define that's not in the specification. This feature allows yetl to be very flexible for any additional requirement that you may have.</p> <p>Example:</p> <pre><code>custom_properties:\n    # define an affinity group to process tables on the same job clusters\n    process_group: 1\n    # define the days to retain the data for after which it is archived or deleted\n    rentention_days: 365\n</code></pre> Yet Another ETL Framework"}]}